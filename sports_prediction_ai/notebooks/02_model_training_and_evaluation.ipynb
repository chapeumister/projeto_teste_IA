{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model Training, Evaluation, and Prediction Pipeline\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Training a simple model using `model_training.py`.\n",
    "2. Evaluating the model using `model_evaluation.py`.\n",
    "3. Running the end-to-end daily prediction pipeline using `prediction_pipeline.py`.\n",
    "\n",
    "**Note:** This notebook uses dummy data/features for model training and prediction as feature engineering is not fully implemented. The prediction pipeline also requires `FOOTBALL_DATA_API_KEY` for fetching live matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ‡§ï‡•ç‡§∞‡•â‡§∏-‡§µ‡•ç‡§π‡•Ö‡§≤‡§ø‡§°‡•á‡§∂‡§®‡§∏‡§π ‡§Ö‡§ß‡§ø‡§ï ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§® (Cross-Validation for Robust Evaluation)\n",
    "\n",
    "Standard train/test split is useful, but cross-validation provides a more robust estimate of model performance by training and testing on different subsets of the data.\n",
    "The `get_cross_val_metrics` function from `model_evaluation.py` can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained_model and not X_train.empty:\n",
    "    print(\"\\n--- Performing Cross-Validation on the Training Data ---\")\n",
    "    # Ensure X_train and y_train are available from the model training step\n",
    "    # The get_cross_val_metrics function is already imported at the top of the notebook.\n",
    "    cv_metrics = get_cross_val_metrics(trained_model, X_train, y_train, cv=3) # Using cv=3 for speed\n",
    "    print(\"\\nCross-Validation Metrics (on training data):\")\n",
    "    for metric, value in cv_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping cross-validation as model was not trained or X_train is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on Random Forest Training with GridSearchCV:\n",
    "The `train_random_forest` function called below now incorporates `GridSearchCV` for hyperparameter optimization. \n",
    "This means it will search through a predefined set of hyperparameter combinations to find the best performing Random Forest model based on cross-validation. \n",
    "As a result, this training step might take longer to execute compared to a simple model fit. The best model found will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "try:\n",
    "    from model_training import split_data, train_random_forest, load_model # Using Random Forest as example\n",
    "    from model_evaluation import get_classification_metrics, plot_confusion_matrix, get_cross_val_metrics # Added get_cross_val_metrics\n",
    "    # Ensure prediction_pipeline can be imported to get its functions\n",
    "    from prediction_pipeline import predict_daily_matches, generate_features_for_prediction \n",
    "    # from data_collection import get_matches_with_fallback # Not directly used here, but good to ensure it's available\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"Make sure 'src' is in sys.path and __init__.py files are present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training (Example with Dummy Data)\n",
    "\n",
    "We'll create a synthetic dataset and train a Random Forest model. In a real scenario, this data would come from `01_data_collection_and_preprocessing.ipynb` after extensive feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for training\n",
    "num_samples = 200\n",
    "data = {\n",
    "    'feature1': np.random.rand(num_samples),\n",
    "    'feature2': np.random.rand(num_samples) * 10,\n",
    "    'feature3': np.random.randint(0, 5, num_samples),\n",
    "    \n",
    "    # New comprehensive form features (dummy values)\n",
    "    # Home team\n",
    "    'home_form_overall_W': np.random.randint(0, 4, num_samples), \n",
    "    'home_form_overall_D': np.random.randint(0, 2, num_samples),\n",
    "    'home_form_overall_games_played': np.full(num_samples, 5),\n",
    "    \n",
    "    'home_form_home_W': np.random.randint(0, 3, num_samples),\n",
    "    'home_form_home_D': np.random.randint(0, 2, num_samples),\n",
    "    'home_form_home_games_played': np.random.randint(2, 6, num_samples), # Random home games played (2-5)\n",
    "\n",
    "    'home_form_away_W': np.random.randint(0, 3, num_samples),\n",
    "    'home_form_away_D': np.random.randint(0, 2, num_samples),\n",
    "    'home_form_away_games_played': np.random.randint(2, 6, num_samples), # Random away games played (2-5)\n",
    "\n",
    "    # Away team\n",
    "    'away_form_overall_W': np.random.randint(0, 4, num_samples),\n",
    "    'away_form_overall_D': np.random.randint(0, 2, num_samples),\n",
    "    'away_form_overall_games_played': np.full(num_samples, 5),\n",
    "\n",
    "    'away_form_home_W': np.random.randint(0, 3, num_samples),\n",
    "    'away_form_home_D': np.random.randint(0, 2, num_samples),\n",
    "    'away_form_home_games_played': np.random.randint(2, 6, num_samples),\n",
    "\n",
    "    'away_form_away_W': np.random.randint(0, 3, num_samples),\n",
    "    'away_form_away_D': np.random.randint(0, 2, num_samples),\n",
    "    'away_form_away_games_played': np.random.randint(2, 6, num_samples),\n",
    "    \n",
    "    'result_label': np.random.choice([0, 1, 2], num_samples, p=[0.45, 0.25, 0.30]) \n",
    "}\n",
    "training_df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure consistent L (Losses) based on W, D, and games_played for all form categories\n",
    "form_categories_notebook = [\n",
    "    'home_form_overall', 'home_form_home', 'home_form_away',\n",
    "    'away_form_overall', 'away_form_home', 'away_form_away'\n",
    "]\n",
    "for cat_prefix in form_categories_notebook:\n",
    "    w_col = f'{cat_prefix}_W'\n",
    "    d_col = f'{cat_prefix}_D'\n",
    "    gp_col = f'{cat_prefix}_games_played'\n",
    "    l_col = f'{cat_prefix}_L'\n",
    "    # Ensure W + D <= games_played before calculating L\n",
    "    training_df[w_col] = training_df.apply(lambda row: min(row[w_col], row[gp_col]), axis=1)\n",
    "    training_df[d_col] = training_df.apply(lambda row: min(row[d_col], row[gp_col] - row[w_col]), axis=1)\n",
    "    training_df[l_col] = training_df.apply(lambda row: row[gp_col] - row[w_col] - row[d_col], axis=1)\n",
    "\n",
    "print(\"Dummy Training Data Head (with comprehensive form features):\")\n",
    "print(training_df.head())\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(training_df['result_label'].value_counts(normalize=True))\n",
    "\n",
    "feature_columns = [\n",
    "    'feature1', 'feature2', 'feature3',\n",
    "    'home_form_overall_W', 'home_form_overall_D', 'home_form_overall_L', 'home_form_overall_games_played',\n",
    "    'home_form_home_W', 'home_form_home_D', 'home_form_home_L', 'home_form_home_games_played',\n",
    "    'home_form_away_W', 'home_form_away_D', 'home_form_away_L', 'home_form_away_games_played',\n",
    "    'away_form_overall_W', 'away_form_overall_D', 'away_form_overall_L', 'away_form_overall_games_played',\n",
    "    'away_form_home_W', 'away_form_home_D', 'away_form_home_L', 'away_form_home_games_played',\n",
    "    'away_form_away_W', 'away_form_away_D', 'away_form_away_L', 'away_form_away_games_played'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on XGBoost Training (Optional):\n",
    "While this notebook primarily trains and uses a Random Forest model, the `model_training.py` script also contains a `train_xgboost` function. \n",
    "This function has been enhanced to use **early stopping**. It internally splits its training data to create a validation set, \n",
    "allowing it to stop training if performance on the validation set doesn't improve for a set number of rounds (e.g., 10). \n",
    "This helps prevent overfitting and can find a more optimal number of boosting rounds. The `verbose=False` parameter is used to keep the output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "if not training_df.empty:\n",
    "    # The split_data function in model_training.py uses all columns except target as features.\n",
    "    # So, X_train will automatically include the new form features added to training_df.\n",
    "    X_train, X_test, y_train, y_test = split_data(training_df, target_column='result_label', test_size=0.25)\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "    print(f\"Features in X_train: {X_train.columns.tolist()}\")\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    rf_model_filename = \"example_rf_model_with_form.pkl\" # New model name\n",
    "    trained_model = train_random_forest(X_train, y_train, model_filename=rf_model_filename)\n",
    "    print(f\"\\nModel trained: {trained_model}\")\n",
    "else:\n",
    "    print(\"Training DataFrame is empty. Skipping training and evaluation.\")\n",
    "    X_test, y_test, trained_model = pd.DataFrame(), pd.Series(dtype='float64'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### üìù Verifying Training Performance (Checklist Item)\n",
    "\n",
    "According to the checklist (`üîç Verificar se Est√° Treinando Certo`):\n",
    "*   **Confirma se a fun√ß√£o de perda (loss) est√° diminuindo durante o treino.** (Confirm if the loss function is decreasing during training.)\n",
    "*   **Avalia se a acur√°cia (ou outra m√©trica) est√° subindo nos dados de valida√ß√£o.** (Evaluate if accuracy (or another metric) is increasing on validation data.)\n",
    "\n",
    "**How to check:**\n",
    "*   **Loss Decreasing:** If you were training a model like XGBoost with an `eval_set`, you would typically see the loss printed for each round. For scikit-learn models like RandomForest, direct epoch-by-epoch loss is not usually displayed during `.fit()`. However, improvements in metrics on a validation set after tuning (e.g., more trees, different depth) indirectly indicate learning. If using XGBoost with `verbose=True` (or a callback) during training with an `eval_set`, you'd monitor the printed validation loss (e.g., `validation_0-mlogloss`).\n",
    "*   **Accuracy Increasing (on Validation):** After training, the subsequent \"Model Evaluation\" section will calculate metrics on the test set. This is a key indicator. If you had a separate validation set split before training, you would run evaluation metrics on that. For iterative models or during hyperparameter tuning, you would look for the validation accuracy/metric to improve.\n",
    "\n",
    "The cell below evaluates the model on the test set. Pay attention to the accuracy and other metrics reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained_model and not X_test.empty:\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    # Ensure X_test has the same features as X_train (split_data handles this)\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    y_prob = trained_model.predict_proba(X_test)\n",
    "\n",
    "    class_labels = sorted(training_df['result_label'].unique())\n",
    "    target_names = [\"Home Win (0)\", \"Draw (1)\", \"Away Win (2)\"] # More descriptive\n",
    "\n",
    "    metrics = get_classification_metrics(y_test, y_pred, y_prob, average='weighted', labels=class_labels, target_names=target_names)\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=class_labels, display_labels=target_names, filename=\"example_notebook_cm_with_form.png\")\n",
    "else:\n",
    "    print(\"Model not trained or test data is empty. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### ü©∫ Checking for Overfitting (Checklist Item)\n",
    "\n",
    "According to the checklist (`üîç Verificar se Est√° Treinando Certo`):\n",
    "*   **Checa se o modelo n√£o est√° apenas decorando os dados (overfitting).** (Check if the model is not just memorizing the data (overfitting).)\n",
    "\n",
    "**How to check:**\n",
    "*   **Compare Training vs. Test Performance:** Overfitting occurs when a model performs exceptionally well on the training data but poorly on unseen data (like the test set or a validation set).\n",
    "    *   To do this thoroughly, you would need to:\n",
    "        1.  Train your model.\n",
    "        2.  Evaluate it on the **training data** (e.g., `trained_model.predict(X_train)` and then use `get_classification_metrics`).\n",
    "        3.  Compare these training metrics with the **test set metrics** calculated above.\n",
    "*   **Signs of Overfitting:**\n",
    "    *   Training accuracy is very high (e.g., 95-100%).\n",
    "    *   Test accuracy is significantly lower than training accuracy.\n",
    "    *   The model might be too complex (e.g., too many features, too deep trees in Random Forest).\n",
    "*   **Addressing Overfitting (General Techniques mentioned in checklist):**\n",
    "    *   Use simpler models.\n",
    "    *   Reduce feature dimensionality.\n",
    "    *   Regularization (e.g., L1/L2 for Logistic Regression, `alpha`/`lambda` in XGBoost).\n",
    "    *   For tree-based models: prune trees, limit max depth, increase minimum samples per leaf.\n",
    "    *   Use techniques like Dropout (for Neural Networks).\n",
    "    *   Use Cross-Validation for more robust evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Daily Prediction Pipeline\n",
    "\n",
    "This part uses the `prediction_pipeline.py` script. Key changes to be aware of:\n",
    "- The pipeline now incorporates **team form features**.\n",
    "- It uses `get_matches_with_fallback` which can use mock data if live data fails for football-data.org.\n",
    "- `generate_features_for_prediction` (called by `predict_daily_matches`) uses `engineer_form_features`.\n",
    "- `engineer_form_features` requires **historical match data**. The pipeline attempts to load this from `data/historical_matches_sample.csv`. If not found, it uses an internal minimal dummy dataset.\n",
    "\n",
    "**Requires:**\n",
    "- A trained model file (e.g., `example_rf_model_with_form.pkl` saved from the step above). The model must be trained with the same features the pipeline generates (including form features and any other base features like 'feature1', 'feature2', 'feature3' if the pipeline's dummy data includes them).\n",
    "- `FOOTBALL_DATA_API_KEY` environment variable for fetching actual daily matches.\n",
    "- Optionally, `APISPORTS_API_KEY` if you want to test with that data source via the pipeline script (though the notebook call below uses football-data.org by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "api_key_notebook = os.getenv(\"FOOTBALL_DATA_API_KEY\", \"YOUR_API_TOKEN\")\n",
    "\n",
    "print(f\"Running prediction pipeline for date: {today_str}\")\n",
    "print(f\"Using model: {rf_model_filename}\") # This should be the model trained with form features\n",
    "print(f\"API Key for football-data.org available: {'Yes' if api_key_notebook != 'YOUR_API_TOKEN' else 'No (will not fetch real data, mock fallback might be used)'}\")\n",
    "\n",
    "model_to_use_in_pipeline = rf_model_filename \n",
    "model_path_pipeline = os.path.join(project_root, 'models', model_to_use_in_pipeline)\n",
    "\n",
    "if not os.path.exists(model_path_pipeline):\n",
    "    print(f\"ERROR: Model {model_to_use_in_pipeline} not found at {model_path_pipeline}.\")\n",
    "    print(\"Please ensure the model from training step was saved correctly or use an existing model file.\")\n",
    "else:\n",
    "    # The prediction_pipeline.py's generate_features_for_prediction now calls engineer_form_features.\n",
    "    # It also adds dummy 'feature1', 'feature2', 'feature3' for consistency with model_training.py.\n",
    "    # Our trained model (example_rf_model_with_form.pkl) includes these features.\n",
    "    print(\"\\nPipeline will attempt to load historical data from '../data/historical_matches_sample.csv' or use a fallback.\")\n",
    "    \n",
    "    # Call predict_daily_matches, setting use_mock_data_if_unavailable to False by default for this notebook example.\n",
    "    # Set to True if you want to demonstrate/test the fallback mechanism when live data might fail.\n",
    "    predict_daily_matches(date_str=today_str, \n",
    "                          model_filename=model_to_use_in_pipeline, \n",
    "                          api_key=api_key_notebook, \n",
    "                          source_api='football-data', \n",
    "                          use_mock_data_if_unavailable=False)\n",
    "    \n",
    "    # Example of how to call it if you want to enable the mock data fallback:\n",
    "    # print(\"\\n--- Example: Running pipeline with mock data fallback enabled (if API fails/returns no data) ---\")\n",
    "    # predict_daily_matches(date_str=today_str, \n",
    "    #                       model_filename=model_to_use_in_pipeline, \n",
    "    #                       api_key=api_key_notebook, \n",
    "    #                       source_api='football-data', \n",
    "    #                       use_mock_data_if_unavailable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Considerations for Real-World Usage:\n",
    "\n",
    "1.  **Feature Consistency**: The features generated by `generate_features_for_prediction` in `prediction_pipeline.py` (which now includes form features and potentially other base features) *must* exactly match the features used to train the loaded model. The examples are aligned, but custom models need careful handling.\n",
    "2.  **Historical Data**: The quality and availability of historical data (like `data/historical_matches_sample.csv`) are crucial for meaningful form features. The current sample CSV is very small.\n",
    "3.  **Model Retraining**: Models should be periodically retrained with new data.\n",
    "4.  **API Key Management**: Securely manage API keys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
