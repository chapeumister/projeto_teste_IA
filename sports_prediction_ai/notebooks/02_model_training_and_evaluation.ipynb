{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model Training, Evaluation, and Prediction Pipeline\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Training a simple model using `model_training.py`.\n",
    "2. Evaluating the model using `model_evaluation.py`.\n",
    "3. Running the end-to-end daily prediction pipeline using `prediction_pipeline.py`.\n",
    "\n",
    "**Note:** This notebook uses dummy data/features for model training and prediction as feature engineering is not fully implemented. The prediction pipeline also requires `FOOTBALL_DATA_API_KEY` for fetching live matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "try:\n",
    "    from model_training import split_data, train_random_forest, load_model # Using Random Forest as example\n",
    "    from model_evaluation import get_classification_metrics, plot_confusion_matrix\n",
    "    from prediction_pipeline import predict_daily_matches, generate_features_for_prediction # For prediction part\n",
    "    # from data_preprocessing import preprocess_match_data # If we need to re-process some data\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"Make sure 'src' is in sys.path and __init__.py files are present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training (Example with Dummy Data)\n",
    "\n",
    "We'll create a synthetic dataset and train a Random Forest model. In a real scenario, this data would come from `01_data_collection_and_preprocessing.ipynb` after extensive feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for training\n",
    "num_samples = 200\n",
    "data = {\n",
    "    'feature1': np.random.rand(num_samples),\n",
    "    'feature2': np.random.rand(num_samples) * 10,\n",
    "    'feature3': np.random.randint(0, 5, num_samples),\n",
    "    # Target: 0 for Home Win, 1 for Draw, 2 for Away Win\n",
    "    'result_label': np.random.choice([0, 1, 2], num_samples, p=[0.45, 0.25, 0.30]) \n",
    "}\n",
    "training_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dummy Training Data Head:\")\n",
    "print(training_df.head())\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(training_df['result_label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "if not training_df.empty:\n",
    "    X_train, X_test, y_train, y_test = split_data(training_df, target_column='result_label', test_size=0.25)\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    # The model is saved by the train_random_forest function in ../models/ directory\n",
    "    rf_model_filename = \"example_rf_model.pkl\"\n",
    "    trained_model = train_random_forest(X_train, y_train, model_filename=rf_model_filename)\n",
    "    print(f\"\\nModel trained: {trained_model}\")\n",
    "else:\n",
    "    print(\"Training DataFrame is empty. Skipping training and evaluation.\")\n",
    "    # Initialize to avoid errors if cells below are run\n",
    "    X_test, y_test, trained_model = pd.DataFrame(), pd.Series(dtype='float64'), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained_model and not X_test.empty:\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    y_prob = trained_model.predict_proba(X_test)\n",
    "\n",
    "    # Define class labels for report (consistent with dummy data: 0, 1, 2)\n",
    "    class_labels = sorted(training_df['result_label'].unique())\n",
    "    target_names = [f\"Class {l}\" for l in class_labels] # e.g., \"Class 0\", \"Class 1\", \"Class 2\"\n",
    "    # Or more descriptive: target_names = [\"Home Win\", \"Draw\", \"Away Win\"]\n",
    "\n",
    "    metrics = get_classification_metrics(y_test, y_pred, y_prob, average='weighted', labels=class_labels, target_names=target_names)\n",
    "    # print(\"\\nEvaluation Metrics:\", metrics)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    # Ensure evaluation_reports directory exists (model_evaluation.py should create it)\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=class_labels, display_labels=target_names, filename=\"example_notebook_cm.png\")\n",
    "else:\n",
    "    print(\"Model not trained or test data is empty. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Daily Prediction Pipeline\n",
    "\n",
    "This part uses the `prediction_pipeline.py` script to fetch today's matches, preprocess them (minimal for now), generate dummy features, load the trained model, and predict outcomes.\n",
    "\n",
    "**Requires:**\n",
    "- A trained model file (e.g., `example_rf_model.pkl` saved from the step above or any model in `models/`).\n",
    "- `FOOTBALL_DATA_API_KEY` environment variable for fetching actual matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "api_key_notebook = os.getenv(\"FOOTBALL_DATA_API_KEY\", \"YOUR_API_TOKEN\")\n",
    "\n",
    "print(f\"Running prediction pipeline for date: {today_str}\")\n",
    "print(f\"Using model: {rf_model_filename}\")\n",
    "print(f\"API Key available: {'Yes' if api_key_notebook != 'YOUR_API_TOKEN' else 'No (will not fetch real data)'}\")\n",
    "\n",
    "# Ensure the model used here is the one trained or a valid one from the models folder\n",
    "model_to_use_in_pipeline = rf_model_filename \n",
    "# Check if the model exists\n",
    "model_path_pipeline = os.path.join(project_root, 'models', model_to_use_in_pipeline)\n",
    "if not os.path.exists(model_path_pipeline):\n",
    "    print(f\"ERROR: Model {model_to_use_in_pipeline} not found at {model_path_pipeline}.\")\n",
    "    print(\"Please ensure the model from training step was saved correctly or use an existing model file.\")\n",
    "else:\n",
    "    # The prediction_pipeline.py uses its own feature generation logic.\n",
    "    # The `generate_features_for_prediction` in that script creates dummy features ('feature1', 'feature2', 'feature3').\n",
    "    # Our dummy training_df also used these feature names, so it should be compatible for this example.\n",
    "    predict_daily_matches(date_str=today_str, model_filename=model_to_use_in_pipeline, api_key=api_key_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Considerations for Real-World Usage:\n",
    "\n",
    "1.  **Feature Consistency**: The features generated by `generate_features_for_prediction` in `prediction_pipeline.py` *must* exactly match the features used to train the loaded model. This is a critical point for the pipeline to work correctly.\n",
    "2.  **Data Availability**: Fetching historical data for feature engineering (e.g., team form, H2H) can be complex and API-dependent.\n",
    "3.  **Model Retraining**: Models should be periodically retrained with new data to maintain performance.\n",
    "4.  **API Key Management**: Securely manage API keys (e.g., using environment variables or a config file, not hardcoded)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
